{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "needed-requirement",
   "metadata": {},
   "source": [
    "# CS4487 - Group Project - Group 22\n",
    "\n",
    "Name List:  \n",
    "CHENG Ho Man 56208961, CHENG Hong Wai 56216309, CHONG Chun Yu 56225263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library installation\n",
    "# pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed116c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# import torchvision module to handle image manipulation\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c44c3",
   "metadata": {},
   "source": [
    "In here, we will input dataset and perform data augmentation. For example, fliping, rotation, color enhancement.\n",
    "Then, all transformed data will be combined into one dataset which will have a size of 12000*9 = 108000 photos.\n",
    "\n",
    "The structure of the test data should be the same as the sample dataset provided by TA Team. There should be two folders storing original and manipulated data separately.\n",
    "\n",
    "\n",
    "    |--- data\n",
    "\n",
    "        |--- test\n",
    "\n",
    "            |--- original\n",
    "\n",
    "                |--- 0001.png\n",
    "\n",
    "                |--- ...\n",
    "\n",
    "            |--- manipulated\n",
    "\n",
    "                |--- DF_0001.png\n",
    "\n",
    "                |--- ...\n",
    "\n",
    "        |--- train\n",
    "\n",
    "            |--- original\n",
    "\n",
    "                |--- 0.png\n",
    "\n",
    "                |--- ...\n",
    "\n",
    "            |--- manipulated\n",
    "\n",
    "                |--- DF_0.png\n",
    "\n",
    "                |--- ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f751a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize \n",
    "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],std=[0.2023, 0.1994, 0.2010]) \n",
    "\n",
    "# original data\n",
    "data_transform = transforms.Compose([transforms.ToTensor(),normalize])\n",
    "# data augmentation\n",
    "data_transform2 = transforms.Compose([transforms.ToTensor(),transforms.RandomHorizontalFlip(100),normalize])\n",
    "data_transform3 = transforms.Compose([transforms.ToTensor(),transforms.ColorJitter(contrast=1),normalize])\n",
    "data_transform4 = transforms.Compose([transforms.ToTensor(),transforms.RandomHorizontalFlip(100),transforms.ColorJitter(contrast=0.7),normalize])\n",
    "data_transform5 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(degrees=(0, 25)),normalize])\n",
    "data_transform6 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(degrees=(335, 360)),normalize])\n",
    "data_transform7 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(degrees=(315, 360)),transforms.RandomHorizontalFlip(100),transforms.ColorJitter(contrast=0.7),normalize])\n",
    "data_transform8 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(degrees=(0, 45)),transforms.RandomHorizontalFlip(100),transforms.ColorJitter(contrast=0.7),normalize])\n",
    "data_transform9 = transforms.Compose([transforms.ToTensor(),transforms.RandomPerspective(distortion_scale=0.5, p=1.0),normalize])\n",
    " \n",
    "    \n",
    "# import dataset\n",
    "train_dir = './data/train'\n",
    "train_datafull = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform, \n",
    "                                  target_transform=None)\n",
    "\n",
    "train_data2 = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform2, \n",
    "                                  target_transform=None)\n",
    "\n",
    "train_data3 = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform3, \n",
    "                                  target_transform=None)\n",
    "train_data4 = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform4, \n",
    "                                  target_transform=None)\n",
    "\n",
    "train_data5 = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform5, \n",
    "                                  target_transform=None)\n",
    "\n",
    "train_data6 = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform6, \n",
    "                                  target_transform=None)\n",
    "\n",
    "\n",
    "train_data7 = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform7, \n",
    "                                  target_transform=None)\n",
    "\n",
    "\n",
    "train_data8 = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform8, \n",
    "                                  target_transform=None)\n",
    "\n",
    "train_data9 = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform9, \n",
    "                                  target_transform=None)\n",
    "\n",
    "conbine = train_datafull,train_data2,train_data3,train_data4,train_data5,train_data6,train_data7,train_data8,train_data9\n",
    "train_data = torch.utils.data.ConcatDataset(conbine)\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6016b71",
   "metadata": {},
   "source": [
    "Here are some of the modified training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data3[1][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e06226",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data9[19][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data6[1][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d3c35",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In our studies, we found that our CNN model has a higher accuracy than pre-trained online EfficientNet, Resnet model. Therefore, we choose CNN as the model in our project.\n",
    "\n",
    "We used CNN to implement the model. We testd the model with 2,3,4 conv layers, 2,3 pool layers. We tried different layers of CNN and finally come out with a most efficient CNN model. There are 4 conv layers, 4 pooling layers and 3 linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,6,4)            # 3@299*299 -> 6@296*296\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # 6@296*296 -> 6@148*148\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6,12,5)           # 6@148*148 -> 12@144*144    \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2) # 12@144*144 -> 12@72*72\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(12,24,5)          # 12@72*72 -> 24@68*68\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2) # 24@68*68 -> 24@34*34\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(24,48,5)          # 24@34*34 -> 48@30*30\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2) # 48@30*30 -> 48@15*15\n",
    "\n",
    "        self.fc1 = nn.Linear(48*15*15,48*15)\n",
    "        self.fc2 = nn.Linear(48*15,48)\n",
    "        self.fc3 = nn.Linear(48,2)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = self.pool1(F.relu(self.conv1(t)))\n",
    "        t = self.pool2(F.relu(self.conv2(t)))\n",
    "        t = self.pool3(F.relu(self.conv3(t)))\n",
    "        t = self.pool4(F.relu(self.conv4(t)))\n",
    "        \n",
    "        t = torch.flatten(t,1)\n",
    "        \n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.fc3(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75eb6c",
   "metadata": {},
   "source": [
    "### Reminder\n",
    "\n",
    "We use cuda to run the model instead of cpu. Therefore, there may be error if the use of devices is different during TA testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d9a26",
   "metadata": {},
   "source": [
    "## Training part\n",
    "\n",
    "Here are the most effective hyper-parameters we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 35\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad054c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "model = model.cuda() # run in GPU\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    count = 0\n",
    "    for img, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        labels = labels.to(device)\n",
    "        img = img.to(device)\n",
    "        preds = model(img)\n",
    "        \n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted_train = torch.max(preds.data, 1)[1]\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels).float().sum()\n",
    "\n",
    "    print ('Train Epoch [{}/{}], Loss: {:.3f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    print ('Training accuracy: {} %'.format(100 * correct_train / total_train))\n",
    "    \n",
    "# save the model \n",
    "torch.save(model.state_dict(), 'group22_cnn.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e01d3e",
   "metadata": {},
   "source": [
    "This CNN model has the highest training accuracy among all CNN models we have implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0695f1",
   "metadata": {},
   "source": [
    "## Testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for TA\n",
    "test_dir = \"./data/test\" # insert data path here\n",
    "\n",
    "# transforms \n",
    "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],std=[0.2023, 0.1994, 0.2010]) \n",
    "data_transform = transforms.Compose([transforms.ToTensor(),normalize])\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                  transform=data_transform, \n",
    "                                  target_transform=None)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=100, shuffle=True)\n",
    "\n",
    "model_test = Network().to(device)\n",
    "model_test.load_state_dict(torch.load('group22_cnn.ckpt'))\n",
    "\n",
    "model_test.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    target_num = torch.zeros((1, 2))\n",
    "    predict_num = torch.zeros((1, 2))\n",
    "    acc_num = torch.zeros((1, 2))\n",
    "    for img, labels in test_loader:\n",
    "        img = img.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model_test(img)\n",
    "        predicted = torch.max(preds.data, 1)[1]\n",
    "    \n",
    "        pre_mask = torch.zeros(preds.size()).scatter_(1, predicted.cpu().view(-1, 1), 1.)\n",
    "        predict_num += pre_mask.sum(0)\n",
    "        tar_mask = torch.zeros(preds.size()).scatter_(1, labels.data.cpu().view(-1, 1), 1.)\n",
    "        target_num += tar_mask.sum(0)\n",
    "        acc_mask = pre_mask * tar_mask \n",
    "        acc_num += acc_mask.sum(0)\n",
    "\n",
    "    recall = acc_num / target_num\n",
    "    precision = acc_num / predict_num\n",
    "    F1 = 2 * recall * precision / (recall + precision)\n",
    "    accuracy = 100. * acc_num.sum(1) / target_num.sum(1)\n",
    "\n",
    "    print('Test Acc {}, recal {}, precision {}, F1-score {}'.format(accuracy, recall, precision, F1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
